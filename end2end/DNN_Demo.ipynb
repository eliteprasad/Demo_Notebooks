{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Keras DNN model\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "1. Create input layers for raw features\n",
    "1. Create feature columns for inputs\n",
    "1. Create DNN dense hidden layers and output layer\n",
    "1. Build DNN model tying all of the pieces together\n",
    "1. Train and evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJ7ByvoXzpVI"
   },
   "source": [
    "## Set up environment variables and load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user google-cloud-bigquery==1.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Restart your kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kindly ignore the deprecation warnings and incompatibility errors related to google-cloud-storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set environment variables so that we can use them throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"$PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"cloud-training-demos\"  # Replace with your PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ML datasets by sampling using BigQuery\n",
    "\n",
    "We'll begin by sampling the BigQuery data to create smaller datasets. Let's create a BigQuery client that we'll use throughout the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq = bigquery.Client(project = PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to figure out the right way to divide our hash values to get our desired splits. To do that we need to define some values to hash within the module. Feel free to play around with these values to get the perfect combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_mod = 100\n",
    "train_pc = 80.0\n",
    "eval_pc = 10.0\n",
    "\n",
    "train_bucket = int(hashing_mod * train_pc / 100.0)\n",
    "eval_bucket = int(hashing_mod * eval_pc / 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a series of queries to check if our bucketing values result in the correct sizes of each of our dataset splits and then adjust accordingly. Therefore, to make our code more compact and reusable, let's define a function to return the head of a dataframe produced from our queries up to a certain number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_head(query, count=10):\n",
    "    \"\"\"Displays count rows from dataframe head from query.\n",
    "    \n",
    "    Args:\n",
    "        query: str, query to be run on BigQuery, results stored in dataframe.\n",
    "        count: int, number of results from head of dataframe to display.\n",
    "    Returns:\n",
    "        Dataframe head with count number of results.\n",
    "    \"\"\"\n",
    "    df = bq.query(\n",
    "        query + \" LIMIT {limit}\".format(\n",
    "            limit=count)).to_dataframe()\n",
    "\n",
    "    return df.head(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first query, we're going to use the original query above to get our label, features, and columns to combine into our hash which we will use to perform our repeatable splitting. There are only a limited number of years, months, days (possibly) and states in the dataset. Try less or more in the hash and see how it changes results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label, features, and columns to hash and split into buckets\n",
    "cleanup_query = \"\"\"\n",
    "SELECT\n",
    "    feature_1,\n",
    "    feature_2,\n",
    "    feature_3,\n",
    "    feature_n,\n",
    "    year,\n",
    "    month,\n",
    "    IFNULL(state, \"Unknown\") AS state,\n",
    "FROM\n",
    "    samples.mydata\n",
    "WHERE\n",
    "    year > 2000\n",
    "    AND feature_1 > 0\n",
    "    AND feature_2 > 0\n",
    "    AND feature_3 > 0\n",
    "    AND feature_n > 0\n",
    "\"\"\"\n",
    "\n",
    "get_df_head(cleanup_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next query will combine our hash columns and will leave us just with our label, features, and our hash values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_fingerprint_query = \"\"\"\n",
    "SELECT\n",
    "    feature_1,\n",
    "    feature_2,\n",
    "    feature_3,\n",
    "    feature_n,\n",
    "    FARM_FINGERPRINT(\n",
    "        CONCAT(\n",
    "            CAST(year AS STRING),\n",
    "            CAST(month AS STRING),\n",
    "            CAST(state AS STRING)\n",
    "        )\n",
    "    ) AS hash_values\n",
    "FROM\n",
    "    ({cleanup_query})\n",
    "\"\"\".format(cleanup_query=cleanup_query)\n",
    "\n",
    "get_df_head(farm_fingerprint_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next query is going to find the counts of each of the unique x number of `hash_values`. This will be our first step at making actual hash buckets for our split via the `GROUP BY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of records for each hash value generated in the previous step\n",
    "first_bucket = \"\"\"\n",
    "SELECT\n",
    "    hash_values,\n",
    "    COUNT(*) AS num_records\n",
    "FROM\n",
    "    ({farm_fingerprint_query})\n",
    "GROUP BY\n",
    "    hash_values\n",
    "\"\"\".format(farm_fingerprint_query=farm_fingerprint_query)\n",
    "\n",
    "get_df_head(first_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query below performs a second layer of bucketing where now for each of these bucket indices we count the number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second time bucketing operation - expected to create roughly 100 buckets for default hashing_mod\n",
    "second_bucket = \"\"\"\n",
    "SELECT\n",
    "    ABS(MOD(hash_values, {hashing_mod})) AS bucket_index,\n",
    "    SUM(num_records) AS num_records\n",
    "FROM\n",
    "    ({first_bucket})\n",
    "GROUP BY\n",
    "    ABS(MOD(hash_values, {hashing_mod}))\n",
    "\"\"\".format(\n",
    "    first_bucket=first_bucket, hashing_mod=hashing_mod)\n",
    "\n",
    "get_df_head(second_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of records is hard for us to easily understand the split, so normalize the count into percentage of the data in each of the hash buckets in the next query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below query states what percentage of total belongs to a particular bucket\n",
    "percentages = \"\"\"\n",
    "SELECT\n",
    "    bucket_index,\n",
    "    num_records,\n",
    "    CAST(num_records AS FLOAT64) / (\n",
    "    SELECT\n",
    "        SUM(num_records)\n",
    "    FROM\n",
    "        ({second_bucket})) AS percent_records\n",
    "FROM\n",
    "    ({second_bucket})\n",
    "\"\"\".format(second_bucket=second_bucket)\n",
    "\n",
    "get_df_head(percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the range of buckets to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = \"\"\"\n",
    "SELECT\n",
    "    *,\n",
    "    \"train\" AS dataset_name\n",
    "FROM\n",
    "    ({percentages})\n",
    "WHERE\n",
    "    bucket_index >= 0\n",
    "    AND bucket_index < {train_buckets}\n",
    "\"\"\".format(\n",
    "    percentages=percentages,\n",
    "    train_buckets=train_buckets)\n",
    "\n",
    "get_df_head(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the range of buckets to be used evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we pick the buckets for evaluation which will be used periodically during training\n",
    "evaluation = \"\"\"\n",
    "SELECT\n",
    "    *,\n",
    "    \"eval\" AS dataset_name\n",
    "FROM\n",
    "    ({percentages})\n",
    "WHERE\n",
    "    bucket_index >= {train_buckets}\n",
    "    AND bucket_index < {cum_eval_buckets}\n",
    "\"\"\".format(\n",
    "    percentages=percentages,\n",
    "    train_buckets=train_buckets,\n",
    "    cum_eval_buckets=train_buckets + eval_buckets)\n",
    "\n",
    "get_df_head(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the hash buckets to be used for the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, query for the hash buckets for our testing set\n",
    "testing = \"\"\"\n",
    "SELECT\n",
    "    *,\n",
    "    \"test\" AS dataset_name\n",
    "FROM\n",
    "    ({percentages})\n",
    "WHERE\n",
    "    bucket_index >= {cum_eval_buckets}\n",
    "    AND bucket_index < {hashing_mod}\n",
    "\"\"\".format(\n",
    "    percentages=percentages,\n",
    "    cum_eval_buckets=train_buckets + eval_buckets,\n",
    "    hashing_mod=hashing_mod)\n",
    "\n",
    "get_df_head(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below query, `UNION ALL` all of the datasets together so that all three sets of hash buckets will be within one table. Add `dataset_id` so that it can be sorted on in the query after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the training, validation (a.k.a evaluation), and testing dataset statistics\n",
    "union = \"\"\"\n",
    "SELECT\n",
    "    0 AS dataset_id,\n",
    "    *\n",
    "FROM\n",
    "    ({training})\n",
    "UNION ALL\n",
    "SELECT\n",
    "    1 AS dataset_id,\n",
    "    *\n",
    "FROM\n",
    "    ({evaluation})\n",
    "UNION ALL\n",
    "SELECT\n",
    "    2 AS dataset_id,\n",
    "    *\n",
    "FROM\n",
    "    ({testing})\n",
    "\"\"\".format(training=training, evaluation=evaluation, testing=testing)\n",
    "\n",
    "get_df_head(union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, show the final split between train, eval, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final splitting according to percentages and other info\n",
    "split = \"\"\"\n",
    "SELECT\n",
    "    dataset_id,\n",
    "    dataset_name,\n",
    "    SUM(num_records) AS num_records,\n",
    "    SUM(percent_records) AS percent_records\n",
    "FROM\n",
    "    ({union})\n",
    "GROUP BY\n",
    "    dataset_id,\n",
    "    dataset_name\n",
    "ORDER BY\n",
    "    dataset_id\n",
    "\"\"\".format(union=union)\n",
    "\n",
    "get_df_head(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a way to get a well-distributed portion of the data in such a way that the train, eval, test sets do not overlap and takes a subsample of the global splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are subsampling from each of the hash values\n",
    "# The multiple chosen will depend on how much data we have\n",
    "\n",
    "subsample_multiple = 1000\n",
    "\n",
    "splitting_string = \"ABS(MOD(hash_values, {0} * {1}))\".format(subsample_multiple, hashing_mod)\n",
    "\n",
    "def create_data_split_sample(query_string, splitting_string, lo, up):\n",
    "    \"\"\"Creates a dataframe with a sample of a data split.\n",
    "    Args:\n",
    "        query_string: str, query to run to generate splits.\n",
    "        splitting_string: str, modulo string to split by.\n",
    "        lo: float, lower bound for bucket filtering for split.\n",
    "        up: float, upper bound for bucket filtering for split.\n",
    "    Returns:\n",
    "        Dataframe containing data split sample.\n",
    "    \"\"\"\n",
    "    query = \"SELECT * FROM ({0}) WHERE {1} >= {2} and {1} < {3}\".format(\n",
    "        query_string, splitting_string, int(lo), int(up))\n",
    "\n",
    "    df = bq.query(query).to_dataframe()\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = create_data_split_sample(data_query, splitting_string,lo=0, up=train_percent)\n",
    "\n",
    "eval_df = create_data_split_sample(data_query, splitting_string,lo=train_percent, up=train_percent + eval_percent)\n",
    "\n",
    "test_df = create_data_split_sample(data_query, splitting_string, lo=train_percent + eval_percent, up=hashing_mod)\n",
    "\n",
    "print(\"There are {} examples in the train dataset.\".format(len(train_df)))\n",
    "print(\"There are {} examples in the validation dataset.\".format(len(eval_df)))\n",
    "print(\"There are {} examples in the test dataset.\".format(len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, notice that there may be numeric fields that are missing in some rows (the count in Pandas doesn't count missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always crucial to clean raw data before using in machine learning, so have a preprocessing step. Define a `preprocess` function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \"\"\" Preprocess pandas dataframe for augmented babyweight data.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe containing raw data.\n",
    "    Returns:\n",
    "        Pandas dataframe containing preprocessed raw data as well\n",
    "            as simulated no data masking some of the original data. Also, any other\n",
    "            cleaning such as one hot encoding and embedding can be performed before \n",
    "            the processed dataframe is returned by this method \n",
    "    \"\"\"\n",
    "    # Clean up raw data\n",
    "    # Filter out what we don\"t want to use for training\n",
    "    df = df[df.feature_1 > 0]\n",
    "    ...\n",
    "    # Any other operations such as one hot encoding categorical data\n",
    "    # Any embeddings needed to be applied to categorical data can be applied here\n",
    "    # Any other preprocessing steps such as filling absent values with \"Unknown\"\n",
    "    # Assigning default numerical values to missing data in a way that doesn't skew the data\n",
    "    # Any data augmentation related operations\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess(train_df)\n",
    "eval_df = preprocess(eval_df)\n",
    "test_df = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to .csv files \n",
    "\n",
    "In the final versions, we want to read from files, not Pandas dataframes. So, write the Pandas dataframes out as csv files. Using csv files gives one the advantage of shuffling during read. This is important for distributed training because some workers might be slower than others, and shuffling the data helps prevent the same data from being assigned to the slow workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "columns = [\"feature_1\",\n",
    "           \"feature_2\",\n",
    "           \"feature_3\",\n",
    "           \"feature_n\"]\n",
    "\n",
    "# Write out CSV files\n",
    "train_df.to_csv(\n",
    "    path_or_buf=\"train.csv\", columns=columns, header=False, index=False)\n",
    "eval_df.to_csv(\n",
    "    path_or_buf=\"eval.csv\", columns=columns, header=False, index=False)\n",
    "test_df.to_csv(\n",
    "    path_or_buf=\"test.csv\", columns=columns, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wc -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tail *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -5 *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set CSV Columns, label column, and column defaults.\n",
    "\n",
    "Now that the existence of CSV files has been verified, set the following that will be using in the input function.\n",
    "* `CSV_COLUMNS` - Ensure that they are in the same order as in the CSV files\n",
    "* `LABEL_COLUMN` - the header name of the column that is the label. Pop it from the features dictionary.\n",
    "* `DEFAULTS` - a list with the same length as `CSV_COLUMNS`, i.e. there is a default for each column in our CSVs. Each element is a list itself with the default value for that CSV column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of string column headers\n",
    "columns = [\"feature_1\",\n",
    "           \"feature_2\",\n",
    "           \"feature_3\",\n",
    "           ...\n",
    "           \"feature_n\"]\n",
    "\n",
    "# Add string name for label column for example feature_4 (short for feature column 4)\n",
    "LABEL_COLUMN = \"feature_4\"\n",
    "\n",
    "# Set default values for each CSV column as a list of lists. For example\n",
    "\n",
    "DEFAULTS = [[0.0], [\"Unknown\"], [0.0], [\"Unknown\"], [0.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset of features and label from CSV files.\n",
    "\n",
    "Next, write an input_fn to read the data. Use `tf.data.experimental.make_csv_dataset`. This will create a CSV dataset object. However, divide the columns up into features and a label. Do this by applying the map method to the dataset and popping the label column off of the dictionary of feature tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_labels(row_data):\n",
    "    \"\"\"Splits features and labels from feature dictionary.\n",
    "\n",
    "    Args:\n",
    "        row_data: Dictionary of CSV column names and tensor values.\n",
    "    Returns:\n",
    "        Dictionary of feature tensors and label tensor.\n",
    "    \"\"\"\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "\n",
    "    return row_data, label  # features, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size=1, mode='eval'):\n",
    "    \"\"\"Loads dataset using the tf.data API from CSV files.\n",
    "\n",
    "    Args:\n",
    "        pattern: str, file pattern to glob into list of files.\n",
    "        batch_size: int, the number of examples per batch.\n",
    "        mode: 'train' | 'eval' to determine if training or evaluating.\n",
    "    Returns:\n",
    "        `Dataset` object.\n",
    "    \"\"\"\n",
    "    # Make a CSV dataset\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        ignore_errors=True)\n",
    "\n",
    "    # Map dataset to features and label\n",
    "    dataset = dataset.map(map_func=features_and_labels)  # features, label\n",
    "\n",
    "    # Shuffle and repeat for training\n",
    "    if mode == 'train':\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input layers for raw features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_layers():\n",
    "    \"\"\"Creates dictionary of input layers for each feature.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of `tf.Keras.layers.Input` layers for each feature.\n",
    "    \"\"\"\n",
    "    inputs = {\n",
    "        colname: tf.keras.layers.Input(\n",
    "            name=colname, shape=(), dtype=\"float32\")\n",
    "        for colname in [\"feature_1\", \"feature_2\"]}\n",
    "\n",
    "    inputs.update({\n",
    "        colname: tf.keras.layers.Input(\n",
    "            name=colname, shape=(), dtype=\"string\")\n",
    "        for colname in [\"feature_3\", \"feature_5\"]})\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature columns for inputs.\n",
    "\n",
    "Next, define the feature columns. Only dense feature columns can be inputs to a DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def categorical_fc(name, values):\n",
    "    \"\"\"Helper function to wrap categorical feature by indicator column.\n",
    "\n",
    "    Args:\n",
    "        name: str, name of feature.\n",
    "        values: list, list of strings of categorical values.\n",
    "    Returns:\n",
    "        Indicator column of categorical feature.\n",
    "    \"\"\"\n",
    "    cat_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            key=name, vocabulary_list=values)\n",
    "\n",
    "    return tf.feature_column.indicator_column(categorical_column=cat_column)\n",
    "\n",
    "\n",
    "def create_feature_columns():\n",
    "    \"\"\"Creates dictionary of feature columns from inputs.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of feature columns.\n",
    "    \"\"\"\n",
    "    feature_columns = {\n",
    "        colname : tf.feature_column.numeric_column(key=colname)\n",
    "           for colname in [\"feature_1\", \"feature_2\"]\n",
    "    }\n",
    "\n",
    "    feature_columns[\"some_boolean_column\"] = categorical_fc(\n",
    "        \"some_boolean_column\", [\"True\", \"False\", \"Unknown\"])\n",
    "    feature_columns[\"multi_category_feature\"] = categorical_fc(\n",
    "        \"multi_category_feature\", [\"Monday(1)\", \"Tuesday(2)\", \"Wednesday(3)\",\n",
    "                      \"Thursday(4)\", \"Friday(5)\", \"Weekends(6+)\"])\n",
    "\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DNN dense hidden layers and output layer.\n",
    "\n",
    "Create some hidden dense layers beginning with our inputs and end with a dense output layer. This is regression so double check the correctness of output layer activation and that the shape is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_outputs(inputs):\n",
    "    \"\"\"Creates model architecture and returns outputs.\n",
    "\n",
    "    Args:\n",
    "        inputs: Dense tensor used as inputs to model.\n",
    "    Returns:\n",
    "        Dense tensor output from the model.\n",
    "    \"\"\"\n",
    "    # Create two hidden layers of [64, 32]\n",
    "    h1 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"h1\")(inputs)\n",
    "    h2 = tf.keras.layers.Dense(32, activation=\"relu\", name=\"h2\")(h1)\n",
    "\n",
    "    # Final output is a linear activation because this is regression\n",
    "    output = tf.keras.layers.Dense(\n",
    "        units=1, activation=\"linear\", name=\"weight\")(h2)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom evaluation metric.\n",
    "\n",
    "This is regression. Define RMSE of the model on our evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_actual, y_predicted):\n",
    "    \"\"\"Calculates RMSE evaluation metric.\n",
    "\n",
    "    Args:\n",
    "        y_actual: tensor, true labels.\n",
    "        y_predicted: tensor, predicted labels.\n",
    "    Returns:\n",
    "        Tensor with value of RMSE between true and predicted labels.\n",
    "    \"\"\"\n",
    "    return tf.sqrt(tf.reduce_mean((y_predicted - y_actual) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build DNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn_model():\n",
    "    \"\"\"Builds simple DNN using Keras Functional API.\n",
    "\n",
    "    Returns:\n",
    "        `tf.keras.models.Model` object.\n",
    "    \"\"\"\n",
    "    # Create input layer\n",
    "    inputs = create_input_layers()\n",
    "\n",
    "    # Create feature columns\n",
    "    feature_columns = create_feature_columns()\n",
    "\n",
    "    # The constructor for DenseFeatures takes a list of numeric columns\n",
    "    # The Functional API in Keras requires: LayerConstructor()(inputs)\n",
    "    dnn_inputs = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns=feature_columns.values())(inputs)\n",
    "\n",
    "    # Get output of model given inputs\n",
    "    output = get_model_outputs(dnn_inputs)\n",
    "\n",
    "    # Build model and compile it all together\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"Here is our DNN architecture so far:\\n\")\n",
    "model = build_dnn_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the DNN using the Keras plot_model utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model=model, to_file=\"dnn_model.png\", show_shapes=False, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5  \n",
    "NUM_EVALS = 5  \n",
    "NUM_EVAL_EXAMPLES = 10000\n",
    "EVAL_BATCH_SIZE = 1000\n",
    "\n",
    "trainds = load_dataset(\n",
    "    pattern=\"train*\",\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    mode='train')\n",
    "\n",
    "evalds = load_dataset(\n",
    "    pattern=\"eval*\",\n",
    "    batch_size=EVAL_BATCH_SIZE,\n",
    "    mode='eval').take(count=NUM_EVAL_EXAMPLES // EVAL_BATCH_SIZE)\n",
    "\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "logdir = os.path.join(\n",
    "    \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logdir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(\n",
    "    trainds,\n",
    "    validation_data=evalds,\n",
    "    epochs=NUM_EVALS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "nrows = 1\n",
    "ncols = 2\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for idx, key in enumerate([\"loss\", \"rmse\"]):\n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "    plt.plot(history.history[key])\n",
    "    plt.plot(history.history[\"val_{}\".format(key)])\n",
    "    plt.title(\"model {}\".format(key))\n",
    "    plt.ylabel(key)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"mymodel_trained\"\n",
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "EXPORT_PATH = os.path.join(\n",
    "    OUTPUT_DIR, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "tf.saved_model.save(\n",
    "    obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "print(\"Exported trained model to {}\".format(EXPORT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $EXPORT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
